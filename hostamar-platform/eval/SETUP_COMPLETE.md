# Eval Setup Complete ‚úÖ

## What's Ready

### Offline Testing (MOCK_MODE)
- `eval/.env` ‡¶è `MOCK_MODE="true"` ‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶Ü‡¶õ‡ßá
- ‡¶è‡¶ñ‡¶® Foundry access ‡¶õ‡¶æ‡¶°‡¶º‡¶æ‡¶á locally run ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá
- Synthetic responses ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡¶¨‡ßá testing ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø

### Run Now
```powershell
cd "c:\Users\romel\OneDrive\Documents\aiauto\hostamar-platform\eval"
npm install
npm run eval:run
npm run eval:metrics
```

### After Foundry Access
‡¶Ø‡¶ñ‡¶® Foundry portal access ‡¶™‡¶æ‡¶¨‡ßá‡¶®:
1. `eval/.env` ‡¶è `MOCK_MODE="false"` ‡¶ï‡¶∞‡ßÅ‡¶®
2. Foundry ‡¶•‡ßá‡¶ï‡ßá exact model deployment name verify ‡¶ï‡¶∞‡ßÅ‡¶®
3. Re-run: `npm run eval:run`

### Files Created
- `eval/run.js` ‚Üí Mock mode + REST fallback ‡¶∏‡¶π
- `eval/metrics.js` ‚Üí Automatic checks (CTA, length, brand terms)
- `eval/.env` ‚Üí Endpoint + mock flag
- `data/queries.jsonl` ‚Üí 3 sample prompts (retail, clinic, restaurant)
- `.github/workflows/foundry-eval.yml` ‚Üí GitHub Actions pipeline
- `foundry/pipeline.yaml` ‚Üí Foundry pipeline template

### Next Steps
1. **Run eval locally** (mock mode):
   ```powershell
   cd eval
   npm run eval:run
   npm run eval:metrics
   ```
2. **Review outputs**:
   - `eval/outputs/gpt-4o.jsonl`
   - `eval/report.json`
3. **Add more queries** to `data/queries.jsonl`
4. **When Foundry ready**, set `MOCK_MODE="false"` and re-run

## Mock Mode Details
- Generates ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ + English mixed responses
- Includes brand terms (Cloud, AI, Hostamar)
- CTA present (\u09af\u09cb\u0997\u09be\u09af\u09cb\u0997, contact, \u09b8\u09be\u0987\u09a8 \u0986\u09aa)
- Length appropriate (~60s script length)

All set! Run `npm run eval:run` now. üöÄ
